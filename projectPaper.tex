\documentclass[11pt, oneside]{article}   	
\usepackage{geometry}                		
\geometry{letterpaper, margin=1.25in}                   		
\usepackage[parfill]{parskip}    		
\usepackage{graphicx}						
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
%\pagenumbering{gobble}
\usepackage{xcolor}
\usepackage{mdframed}
\usepackage{wrapfig}
\usepackage{multicol}

\usepackage{listings} %For including R code in LaTex
\lstset{language=R,
    basicstyle=\small\ttfamily,
    stringstyle=\color{DarkGreen},
    otherkeywords={0,1,2,3,4,5,6,7,8,9},
    morekeywords={TRUE,FALSE},
    deletekeywords={data,frame,length,as,character},
    keywordstyle=\color{blue},
    commentstyle=\color{DarkGreen},
} %color Scheme for R code in LaTex

%\begin{bmatrix}
%\bigg|_{}^{}        for "evaluated at..."
%\sim		   for tilde



\title{Group Project:\\
\emph{Evaluation of Statistical Model Accuracy}
}
\author{\hspace{0.0cm}\parbox[t][2.5cm][t]{4cm}{
	Bryce Robinette\\
	David Koster\\
}
\parbox[t][2.5cm][t]{4cm}{
	Jacelyn Villalobos\\
	Jacob Ruiz\\
	%Kursten Reznik\\
}
Kursten Reznik 
}
\date{}							

\begin{document}
\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\break
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section*{Introduction}
In this paper, we compare the performance of four statistical models using randomly generated data with normal distribution $N(\mu, \sigma^2)$: The models we are investigating are: K-Nearest Neighbors (KNN), Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA), and the Logistic Regression model (GLM in \textbf\textsf{R}). That is, we analyze these models' accuracy with respect to sample size, number of input variables, the variance of the data, and normalization of the data.\\
\\
We have been given four precise scenarios for which these models are to be tested. We outline them here:\\


\hspace{1.5cm}\parbox[t][2.5cm][t]{8cm}{
	\textbf{Scenario 1}\\
	$\cdot$Sample Size $N = 50.$\\
	$\cdot$Number of Inputs $\rightarrow 2$.\\
	$\cdot$No scaling or normalizing the data.
}
\parbox[t][2.5cm][t]{8cm}{
	\textbf{Scenario 2}\\
	$\cdot$Sample Size $N = 500.$)\\
	$\cdot$Number of Inputs $\rightarrow 2$.\\
	$\cdot$Normalized Data.
}

\hspace{1.5cm}\parbox[t][2.5cm][t]{8cm}{
	\textbf{Scenario 3}\\
	$\cdot$Sample Size $N = 50.$\\
	$\cdot$Number of Inputs $\rightarrow 20$.\\
	$\cdot$No scaling or normalizing the data.
}
\parbox[t][2.5cm][t]{8cm}{
	\textbf{Scenario 4}\\
	$\cdot$Sample Size $N = 500.$)\\
	$\cdot$Number of Inputs $\rightarrow 20$.\\
	$\cdot$Normalized Data.
}

In each of these scenarios, we will look at the accuracy of each model as it relates to the variance of the data. It should also be noted that this paper assumes that the reader is familiar with these statistical models and the programming language \textbf\textsf{R}.

\pagebreak


%\begin{mdframed}[backgroundcolor=lightgray]
%\begin{lstlisting}[language=R]
%data.generate = function(mu1, mu2, s, n, p){
%  y = as.factor(c(rep(1,n), rep(2,n)))
%  M = matrix(NA, nrow = 2*n, ncol = p, byrow = 1)
%  df = data.frame(y)
%  for (i in c(1:p)){
%    M[,i] = c(rnorm(n,mu1,s), rnorm(n,mu2,s))
%  }
%  df = cbind(y,M)
%  df = data.frame(df)
%  return(df)
%}
%\end{lstlisting}
%\end{mdframed}


\section*{Materials and Methods}
To begin, we perform each model with the parameters outlined in the four given scenarios. These four scenarios will give us our blueprint for analysis. Indeed, for each scenario, we run each model against a number of data sets of identical properties but with increasing variance in our randomly generated data. Each of these data sets are also sampled many times in order to return each model's average accuracy for that data's given variance.\\
Moreover, we then evaluate the accuracy of each model as a function of the variance of the data sets in each scenario and plot the results. This methodology should yield the generalized accuracy of the model given the variance of the data.\\
\\
\textcolor{blue}{Furthermore, we compare the accuracy of the models to gain insight into which model should likely be used given a set of data of normal distribution.}
 

\subsection*{Data Generation} 
% Discussion on how we generated our data.
We generate our data such that the response variable $y$ has two classes. That is, $y=1$, or $y=2$ with class means $\mu_1=1$ and $\mu_2=0$, respectively. Then we randomly generate values with distribution $N(\mu, \sigma^2)$1 for our predictors.\\
\\
\textcolor{blue}{Should I include code here to show or just assume that our R-file is sufficient?}\\
\\
It should also be noted that when we have a smaller amount of observations $N$, there is a chance that when sampling the data into testing and training data that we could end up with an imbalanced sample of our response value. To mitigate this, we could sample an approximately equal amount of data with each value of $y$; however, since we are running a multitude of simulations on the data, we can rely on the central limit theorem.\\ 


%\begin{mdframed}[backgroundcolor=lightgray]
%\begin{lstlisting}[language=R]
%data.generate = function(mu1, mu2, s, n, p){
 % y = as.factor(c(rep(1,n), rep(2,n)))
  %M = matrix(NA, nrow = 2*n, ncol = p, byrow = 1)
  %df = data.frame(y)
  %for (i in c(1:p)){
   % M[,i] = c(rnorm(n,mu1,s), rnorm(n,mu2,s))
  %}
  %df = cbind(y,M)
  %df = data.frame(df)
  %return(df)
%}
%\end{lstlisting}
%\end{mdframed}
\subsection*{Choosing our best $K$}
In this section we present our best $k$. The very best $k$. In fact, it is the best $k$ in the history of $k$'s. So good it will blow your mind. Here is how we did it....\\
\\
\textcolor{blue}{WORDS}\\
\\

We simulated our KNN model over a number of generated data sets with differing variances and chose the most common ``best $k$'' that was associated with these particular data sets. We took this common $k$ to be our generalized $k$-value for our following analysis.\\
\\

\pagebreak

\subsection*{Scenario 1}

\begin{wrapfigure}[]{R}{0.5\textwidth}
\vspace{-0.55cm}
\includegraphics[scale=0.4]{Scenario1.png}
\vspace{-.55cm}
\end{wrapfigure}
In this scenario, we compare out models with $N=50$ observations, $p=2$ parameters, and without scaling or normalizing the data.\\
\\
From the resulting figure, we can conclude that all models diminish in accuracy as the variance increases. This is to be expected given that our data was generated randomly without any underlying function dictating the data generating process. Indeed, LDA and Logistic Regression assume a linear relationship between the response variable and the predictors while KNN is non-parametric. QDA is a decent compromise between KNN and any linear model as it assumes a nonlinear relationship between the predictors and the response.\\

In this scenario, the KNN model performs the best. This is to be expected due to the fact that we have chosen a relatively small $k$ for our analysis which makes our particular KNN model more flexible than it would be with a larger $k$-value. In this regard, we migh5 expect our KNN model to perform better under the assumed circumstances for our analysis

\subsection*{Scenario 2}
\begin{wrapfigure}[]{L}{0.5\textwidth}
\vspace{-0.55cm}
\includegraphics[scale=0.35]{Scenario2.png}
\vspace{-.55cm}
\end{wrapfigure}
In this scenario, we take $N=500$, parameters $p=2$, and we scale the data.\\
\\
We immediately see that the KNN model performs the best. Here is where we see the flexible model shine. Indeed, as we know with our brilliant minds, when we are presented with a large number of observations and the value of our predictors is small, we want to choose a more flexible model. Due to the large sample size, we are less likely to over-fit, even with a more flexible model.\\
\\
The LDA, QDA, and Logistic Regression models all perform at approximately the same level of accuracy throughout the data variances. (\emph{like the Backstreet boys $\rightarrow$ TELL ME WHY???})\\
\\
The other three models are less flexible the KNN. This incl;udes QDA. QDA allows for more flexibility but it requires more parameters to estimate the accuracy.\\
\\
\textcolor{blue}{ If you have many classes and not so many sample points, this can be a problem for the QDA model.}\\

\textcolor{blue}{All of our models seem to be doing well; however the flexible nature of the KNN model makes it a superior predictor in this case.}

\textcolor{red}{IDENTICAL Y-AXES/VERTICAL SCALING}



\subsection*{Scenario 3}
\begin{wrapfigure}[]{L}{0.5\textwidth}
\vspace{-0.55cm}
\includegraphics[scale=0.4]{newScenario3.png}
\vspace{-1.0cm}
\end{wrapfigure}

Now we take $N=50$, and increase our predictors to $p=20$. The data is not to be scaled in this scenario.\\
\\
As we can see, this is some bullshit...\\
\\

From the resulting plot, we immediately conclude that an increase in our predictors $p$ has yielded higher model accuracy over increasing variance of the data.\\
\\
I suppose that one may conclude in cases where data variance is high, LDA and QDA may perform better with a larger number of predictors. However, we should be wearing of increasing the number of predictors too high that we start over-fitting and incorporating too much noise in our models.\\
\\

Comparisons of KNN and GLM are to come as their information is made available.\\
\\
$\bullet$ Maybe write something about scenario 3 having a problem with QDA. When we have too small a sample sixe to compare with our predictors, then shit gets whack.\\
\\
\textcolor{blue}{Since we have designated or predictors to be $p=20$ it may result that the QDA performes in a simi.l,ar fation to LDA since we are not allowing the model th convenience of \emph{best predictora}. That is, QDA  (in general) needs a larger amount of predictors, but if the predictors have little significance, then id doeznt contribute the overall model accuracy andf may lead to an overfit and shit and stuff and yeah dawg.}\\
\\
\textcolor{red}{NO... Dr, Chan. we must bhave more data. If you dont like it, its Davids fault}\\
\\
 

\subsection*{Scenario 4}
\begin{wrapfigure}[]{R}{0.45\textwidth}
\vspace{-0.55cm}
\includegraphics[scale=0.4]{newScenario4.png}
\vspace{-.55cm}
\end{wrapfigure}

We now increase our number of observations to $N=500$, keep or parameters $p=20$, and scale our data.\\
\\

\textcolor{blue}{knn will operform a bit worse wityh a larger number of predictors since we awill almosy inevbitably introduce noise into the model.}\\
\\
as the number of predictors increases we will need to use a more inflexible model. Having a high number of predictors can lead to over fitting of the data.\\
\\
\textcolor{blue}{what we wsanted to show her e was when n is large the models will will in a general sense perform better sinse the number oif pdata pointds will help inform the ,odel (\textcolor{blue}{tell me why)}.... however, we should expect toi see the KNN and thre QDA to outperform the linear models due to the fact that with a larger number of predictors, nonlinear and non-parametric mm,odels should outperform linrear models that are less felxible that their nonlinear counterparts. }

%Scaling the data has further increased the average accuracy of the LDA and QDA models. Again, in a not so surprising way, LDA and QDA still follow highly similar paths of accuracy decline as the variance increases.\\










%\pagebreak









%Furthermore, with our particular method of data generation, QDA falls on its face as well. QDA assumes a non-linear result from our response variable when functioned with our parameters.\\




\subsection*{Writing Notes}
$\bullet$\textcolor{green}{Would we have suspected the QDA to perform better because of its ability to accommodate more flexible decision boundaries that could be present in randomly generated data?}\\
\\
$\bullet$The number of parameters needed to estimate QDA increases faster than LDA. The number of parameters estimated in LDA increases linearly with $p$ while that of QDA increases quadratically with $p$. So, from the resulting analysis, we can see that when  we have a larger data set and the number of predictors is large, the Quadratic Discriminant Analysis  performed better  than the linear models due to the fact that with a larger set if data(larger variance) in scenario 4 we are give  a larger value of predictors, which in turn benefits the quadratic model to accept nonlinear relationships between the response variable and the the predictors.,



\pagebreak
\section*{Discussion}




\subsection*{Extensions?}
 Instead of normally distributed data, we could assess performance or accuracy of the models over other distribution sets, such as a uniform distribution.


\pagebreak
\section{Conclusion}



\section{Credits}
McDonald's: Obviously.\\
\\
Alcohol: for its continuing effort to keep me sane.\\
\\
Sir David Attenborough: for the love of all that is good.\\
\\
I dunno... maybe my cat?


\section*{References}
ME... FOOLS... I am the only reference you need.\\
\\
JK seriously give yer references..

\subsection*{GLM (to be rewritten later as part of the scenario analysis)}
Logistic regression is going to have a different set of standards than our other models (except LDA). It is assumed that the reader recollects that Logistic Regression is parametric, while KNN is non-parametric.\\
\\
Use your brain for a moment.... You then realize that LDA and GLM require linearity for any meaningful result. QDA is a decent compromise between KNN and any linear model, (such as GLM), since QDA also assumes non-linearity.\\
\\
Use your eyes and read:\\
KNN will support non-linear solutions where GLM can only support linear solutions. Furthermore, KNN cannot yield confidence levels due to its selection nature where GLM can because it has the property of linearity.\\
\\
Neighborhoods have meaningful weight....\\
\\
In this regard, I have to say at this particular moment, that \emph{Logistic Regression} is a largely unnecessary model when dealing with data generated with a known normal distribution without a further informative function dictating the outcome or response variable.\\
\\
Like, for reel check it motha fuggas..... GLM and LDA are based on identical assumptions of linearity and statistical regression, making one no more informative than the other in our particular cases with randomly generated data derived from $N(\mu, \sigma^2)$.\\
\\
\\
\end{document}